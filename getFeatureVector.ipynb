{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from collections import Counter\n",
      "from nltk import sent_tokenize,word_tokenize,porter,bigrams,pos_tag\n",
      "from nltk.corpus import stopwords\n",
      "import operator\n",
      "from nltk.corpus import wordnet as wn\n",
      "import cPickle as pickle\n",
      "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
      "from sklearn.preprocessing import scale\n",
      "import numpy as np"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def getPickledFile(emotions,fname):\n",
      "    pickle.dump(emotions,open(fname,'wb'))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 18
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def getBagOfWords(youcomments):\n",
      "    bagOfWordsDict = {}\n",
      "    for muvi, comments in youcomments.iteritems():\n",
      "        word_counts = Counter()\n",
      "        for sent in sent_tokenize(comments):\n",
      "            for tag in pos_tag(word_tokenize(sent)):\n",
      "                if tag[0] not in stopwords.words('english'): \n",
      "                    if tag[1] == 'JJ': \n",
      "                        word_counts[(tag[0].lower())+'#'+'a'] += 1\n",
      "                    elif tag[1] == 'RB':\n",
      "                        word_counts[(tag[0].lower())+'#'+'r'] += 1\n",
      "                    elif tag[1] == 'VBG' or tag[1] == 'VBD' or tag[1] == 'VBN' or tag[1] == 'VBZ' or tag[1] == 'VB': \n",
      "                        word_counts[(tag[0].lower())+'#'+'v'] += 1\n",
      "                    elif tag[1] == 'NN':\n",
      "                        word_counts[(tag[0].lower())+'#'+'n'] += 1\n",
      "        bagOfWordsDict[muvi] = word_counts\n",
      "    return bagOfWordsDict "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "wordScores = pickle.load( open( \"wordScore.p\", \"rb\" ) )"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "IN_DIR = 'pickledCommentFiles/'\n",
      "OUT_DIR = 'pickledBOA/'\n",
      "\n",
      "# comment_files = ['comments_50.p','comments_55.p','comments_65.p','comments_100.p','comments_spoo.p','comments_swe.p']\n",
      "\n",
      "comment_files = ['comments_115.p']\n",
      "\n",
      "for c_file in comment_files:\n",
      "    \n",
      "    youComments = pickle.load(open(IN_DIR + c_file, 'rb'))\n",
      "\n",
      "    keys = wordScores.keys()\n",
      "\n",
      "    bow = getBagOfWords(youComments)\n",
      "    for muvi, bag in bow.iteritems():\n",
      "        for w, count in bag.iteritems():\n",
      "            if w in keys:\n",
      "                bag[w] *= wordScores[w]\n",
      "            else:\n",
      "                bag[w] = 0\n",
      "\n",
      "    pos_index = dict()\n",
      "    neg_index = dict()\n",
      "\n",
      "    for muvi, bag in bow.iteritems():\n",
      "        pos_index[muvi] = 0\n",
      "        neg_index[muvi] = 0\n",
      "    \n",
      "\n",
      "    for muvi, bag in bow.iteritems():\n",
      "        ws = 0\n",
      "        for w, count in bag.iteritems():\n",
      "            if count > 0:\n",
      "                ws += 1\n",
      "                pos_index[muvi] += count\n",
      "            elif count < 0:\n",
      "                ws += 1\n",
      "                neg_index[muvi] += (-count)\n",
      "        pos_index[muvi] /= ws\n",
      "        neg_index[muvi] /= ws\n",
      "    getPickledFile(pos_index, c_file + '_positive.p')\n",
      "    getPickledFile(neg_index, c_file + '_negative.p')\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def select_relavant_words(text):\n",
      "    text=word_tokenize(text)\n",
      "    tagged_text = pos_tag(text)\n",
      "    imp_words=[]\n",
      "    for word,tag in tagged_text:\n",
      "        if tag in set(['JJ','JJR','JJS','RB','RBR','RBS','UH','VBG']):\n",
      "            imp_words.append(word)\n",
      "    return ' '.join(imp_words)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 19
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def count_relavant_word_frequencies(youcomments):\n",
      "    print 'counting....'\n",
      "    word_count = Counter()\n",
      "    wc = Counter()\n",
      "    for muvi, comments in youcomments.iteritems():\n",
      "        relevant_words = select_relavant_words(comments)\n",
      "        for sent in sent_tokenize(comments):\n",
      "            for word in word_tokenize(sent):\n",
      "                if word not in stopwords.words('english') and word in relevant_words:\n",
      "                    word_count[word.lower()] += 1\n",
      "                    wc[word.lower()] += 1\n",
      "    word_count = [a for a,b in sorted(word_count.items(),key=lambda x: x[1],reverse=True)]\n",
      "    wc = sorted(wc.items(),key=lambda x: x[1],reverse=True)\n",
      "    print wc\n",
      "#     getPickledFile(word_count,'corpus_word_counts.p')\n",
      "    print 'done counting'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 20
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def get_corpus(youcomments):\n",
      "    corpus = []\n",
      "    for k,v in youcomments.iteritems():\n",
      "        corpus.append(str(v))\n",
      "    getPickledFile(corpus,'corpus.p')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 21
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def getTop_n_words(n):\n",
      "    vocab=pickle.load(open('corpus_word_counts.p','rb'))\n",
      "    return vocab[:n]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 22
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def get_linguistic_feature_vector(corpus,vocab,vect):\n",
      "    print 'getting f vector'\n",
      "    if vect=='tfidf':\n",
      "        vectorizer = TfidfVectorizer(min_df=1,vocabulary=vocab)\n",
      "    else:\n",
      "        vectorizer = CountVectorizer(min_df=1,vocabulary=vocab)\n",
      "    X = vectorizer.fit_transform(corpus)\n",
      "    return X,vectorizer\n",
      "    print 'done: f vector'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 23
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def process_sentiments(features,sentipkl,vectorizer):\n",
      "    print 'getting senti'\n",
      "    senti=pickle.load(open(sentipkl))\n",
      "    names=vectorizer.get_feature_names()\n",
      "    wts=np.ones([features.shape[1]])\n",
      "    for i,name in enumerate(names):\n",
      "        if name+'#a' in senti.keys():\n",
      "            wts[i]=senti[name+'#a']['posScore']\n",
      "        else:\n",
      "            wts[i]=0.1\n",
      "    fwts=np.tile(wts,(features.shape[0],1))\n",
      "    senti_fv=np.multiply(features,fwts)\n",
      "    print 'got senti'\n",
      "    return senti_fv"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 24
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "IN_DIR = 'pickledCommentFiles/'\n",
      "OUT_DIR = 'out/'\n",
      "# comment_files = ['comments_50.p','comments_115.p','comments_55.p','comments_65.p','comments_100.p','comments_spoo.p','comments_swe.p']\n",
      "# commentsDict = {}\n",
      "# for c_file in comment_files:\n",
      "#     yc = pickle.load(open(IN_DIR + c_file, 'rb'))\n",
      "#     commentsDict = dict(commentsDict.items() + yc.items())    \n",
      "# print len(commentsDict)\n",
      "# getPickledFile(commentsDict,'commentsDict.p')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 25
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sentipkl = 'SentiWordNet.p'\n",
      "all_comments = pickle.load(open('commentsDict.p','rb'))\n",
      "count_relavant_word_frequencies(all_comments)\n",
      "get_corpus(all_comments)\n",
      "corpus = pickle.load(open('corpus.p','rb'))\n",
      "vocab = getTop_n_words(120)\n",
      "features,vectorizer = get_linguistic_feature_vector(corpus,vocab,'tfidf')\n",
      "senti_fv = process_sentiments(features.todense(),sentipkl,vectorizer)\n",
      "getPickledFile(senti_fv,OUT_DIR +'fvector.p')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    }
   ],
   "metadata": {}
  }
 ]
}